input {
  tcp {
    port => 514
    type => syslog
    # codec => multiline {
    #   pattern => "^\s"
    #   what => "previous"
    # }
  }
}

filter {

  # first parse out the body from the syslog format
  # we've added tags via the Docker log drivers to identify the
  # specific container and the service identifier. See:
  # https://docs.docker.com/engine/admin/logging/log_tags/
  grok {
    match => {
      "message" => '%{SYSLOG5424PRI:syslog5424_pri}+(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|-) %{WORD:serviceid}/+(?:%{HOSTNAME:containerid}|-)\[+(?:%{POSINT:pid}|-)\]: %{GREEDYDATA:msg}'
      }
  }
  syslog_pri { }

  # failed to match, so parse as error
  if "_grokparsefailure" in [tags] {
    mutate {
      add_tag => "parse_error"
    }
  } else {
      mutate {
        # the raw message is redundant data at this point
        remove_field => [ "message", "@source_host" ]
      }
      mutate {
        # once we've got a valid syslog parse we can discard all this rubbish
        # because the Docker log driver stomped all over anything useful
        remove_field => [
           "syslog_hostname", "syslog_message", "syslog_timestamp",
           "syslog_severity", "syslog_facility_code", "syslog_severity_code",
           "syslog_facility", "syslog5424_pri"
        ]
      }
  }

  # filter to get the application-specific log format. lots of these have
  # their own timestamps, which we'll capture and overwrite the "outermost"
  # timestamp with
  grok {
    # nginx access log
    match => {
      "msg" => '\[%{HTTPDATE:log_timestamp}\] %{WORD:req_id} "%{WORD:http_method} %{URIPATHPARAM:http_request} HTTP/%{NUMBER:http_version}" %{NUMBER:http_code} %{NUMBER:http_bytes_sent} (?:%{QUOTEDSTRING:http_referer}|-) %{IP:client} (?:%{QUOTEDSTRING:http_user_agent}|-)'
    }
    # nginx error msg
    match => {
      "msg" => '(?<log_timestamp>%{YEAR}/%{MONTHNUM}/%{MONTHDAY} %{TIME}) (?<http_timestamp>%{YEAR}/%{MONTHNUM}/%{MONTHDAY} %{TIME})? ?%{GREEDYDATA:msg}'
    }

    # mysql log messages
    match => {
      "msg" => '%{TIMESTAMP_ISO8601:log_timestamp} %{NUMBER} \[%{LOGLEVEL:level}\] ?%{GREEDYDATA:msg}'
    }
    # mysql manage.py log messages
    match => {
      "msg" => '(?<log_timestamp>%{YEAR}/%{MONTHNUM}/%{MONTHDAY} %{TIME})? %{LOGLEVEL:level} manage %{GREEDYDATA:msg}'
    }
    # ContainerPilot log format
    match => {
      "msg" => '(?<log_timestamp>%{YEAR}/%{MONTHNUM}/%{MONTHDAY} %{TIME})? ?%{GREEDYDATA:msg}'
    }

    # catchall
    match => { "msg" => "%{GREEDYDATA:msg}" }

    overwrite => [ "log_timestamp" ]
    overwrite => [ "msg" ]

  } # end grok

  # fortunes application
  json {
    source => "msg"
    add_field => [ "log_timestamp", "%{time}"] # overwrites w/ timestamp from app
    remove_field => [ "time" ]
  }

  # anything other than our fortunes application will not be JSON
  # so we'll ignore this error
  if "_jsonparsefailure" in [tags] {
    mutate { remove_tag => "_jsonparsefailure" }
  }

  # TODO: transform log_timestamp field ("2016/11/01 18:07:31") to @timestamp and remove it
  # TODO: transform level field to consistent loglevel

}

output {
  elasticsearch { hosts => ["elasticsearch"] }
  stdout { codec => rubydebug }
}
